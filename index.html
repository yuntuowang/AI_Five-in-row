<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Magical Gomoku by yuntuowang</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Magical Gomoku</h1>
      <h2 class="project-tagline">--- EECS 349 final project 2016 by Chaofan Yu, Fei Zhao, Yuntuo Wang and Qian Wang</h2>
      <a href="https://github.com/yuntuowang/AI_Five-in-row" class="btn">View on GitHub</a>
      <a href="https://github.com/yuntuowang/AI_Five-in-row/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/yuntuowang/AI_Five-in-row/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <p><strong>Extended final report: <a href="https://drive.google.com/file/d/0Bz9F7OxNRzz6NElyb3RPakVYY2c/view?usp=sharing">Download PDF</a></strong></p>

<h2>
<a id="introduction" class="anchor" href="#introduction" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h2>

<p>Gomoku is an abstract strategy board game. Also called Gobang or Five in a Row, it is traditionally played with Go pieces (black and white stones) on a go board with 19x19 (15x15) intersections. This game is known in several countries under different names. Black plays first if white just lost in the last game, and players alternate in placing a stone of their color on an empty intersection. The winner is the first player to get an unbroken row of five stones horizontally, vertically, or diagonally.</p>

<p><img src="http://imgur.com/xqXwFL9"></p>

<p><strong>Figure 1: Example of Gomoku</strong></p>

<p>Our task is to make a Gomoku game AI which can learn the rules of Gomoku by playing games with itself. It is similar with the AlphaGo machine which is a really popular and amazing application. This kind of machine learning algorithm has a wide application scenario. No matter whichever kind of the game or other problem we deal with, what we need to do is to make the “computer1”, which is the trainee, to play games with the “computer2”, which has all relative experience and rules of Gomoku game, it will learn automatically and get a good result. That is the reason why AlphaGo team and such kind of teams have fast development recently.</p>

<h2>
<a id="design" class="anchor" href="#design" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Design</h2>

<p>Before our implementing any machine learning techniques to train a computer to play Gomoku, we need to create an interface which would allow us to simulate a game of Gomoku. This game involves two kinds of chess, player, AI, rules and so on. We implement game class, AI class, player class, gomoku class with Python by ourselves, without using any existing package. </p>

<p>In the game interface part, we made a very useful and interactive design. Firstly, users can choose to play with the AI which is trained before or to play with another player. The first mode allows the AI to play with itself before to train the model and the second mode tends to enhance the interactive function which allow users to play Gomoku with friends. Then, users can set their specific Gomoku rules, including the size of the chess board (from 3<em>3 to 10</em>10) and the number of chess to win (more than 2). This kind of design is to increase the interest of this game and to show that different rules won’t change the basic algorithm since they have the same mechanism.</p>

<p>In the game process part, we built up a “dealer” system as the original AI, with which users can play when they don’t want to train a better and clever AI. There are three kinds of strategies: attack, defend and neutral. There is a grading system to help AI make decision which kind of strategy it will take in the next step. This system is based on judging the situation, the grades of AI and player and which kind of strategy leads to a higher score.</p>

<p>In the AI part, we use heuristic and reinforcement learning techniques to train the model. After researching reinforcement learning techniques, we decided to use Q-learning to train our simulation player with how to play Gomoku because it learns the state-action value function and the exploration policy.  It is also an off-policy method(an <strong>off-policy</strong> learner learns the value of the optimal policy independently of the agent's actions), which gathers information from partial random moves. Comparing to the traditional Q-learning method, we combine it with the heuristic method. This kind of combination can speed up the learning process since it simplifies the gradient descent learning process. We set five intelligence coefficients to use as alpha parameters and gamma parameters of the heuristic algorithm in three strategies (attack, defend, neutral). At the beginning of the training process, we initialize five coefficients to build up the original model and then make some changes in these five parameters to make a new mode. Then the new model plays with the older one. If the new one wins, we will change the five coefficients to the new version, or we will leave them unchanged in this round.</p>

<h2>
<a id="results-and-analysis" class="anchor" href="#results-and-analysis" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Results and analysis</h2>

<p>To judge the performance of our algorithm, we made our interface play a large number of games and recorded the win percentage of our new model over the older model. Our result shows that our player was clearly learning the rules of the game and refining its strategy after each round of play. Its win percentage went up rapidly within the first few hundred games as shown in the figures. Instead of playing with the original model, we made each new model play game with the old model of the last round. This setting is much more helpful for us to learn whether our model is developing in each round. If we just make new model play with the original version, it will always win even though the model doesn’t develop after several rounds. But based on our situation, the win percentage will increase only if the model develops continuously. </p>

<p>Then we set different initialization values for five coefficients at the beginning and we find that this kind of change will not change the final result of the model parameters. It will also take about hundreds of rounds to get a better result and to be stable finally.</p>

<p>The results of accuracy are attached below. From the results, we can find out the AI model develops fast in about first 3000 rounds and then it is much more stable after 3000 rounds and its accuracy is about 96.3% after 10000 rounds. The detailed result is stored in our “result_gomoku.csv” document.</p>

<p><img src="http://imgur.com/OQCN2Z9"></p>

<p><strong>Figure 2: Win percentage distribution with maximum 3000 rounds</strong></p>

<p><img src="http://imgur.com/sA91zCe"></p>

<p><strong>Figure 3: Win percentage distribution with maximum 10000 rounds</strong></p>

<h2>
<a id="conclusion-and-future-work" class="anchor" href="#conclusion-and-future-work" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Conclusion and Future Work</h2>

<p>By implementing the combination of heuristic and reinforcement algorithm using Python by ourselves, we are able to build a Gomoku AI player which can quickly learn the rules of Gomoku and eventually learns more advanced strategies, and to enable it with a much higher win percentage than its original version. And finally about 100% win percentage means that our model is stable enough in such a size of board size and the specific rules.</p>

<p>In future, we intend to add the traditional gradient descent optimization method to this project and compare the convergence speed of this method with the convergence speed using our heuristic method. And we will try to implement AI with game tree algorithm. Actually, we had a try on it and achieved some goals. We can use game tree algorithm to train a model which is even better than our model using heuristic and reinforcement learning algorithm. But we cannot let it develop continually after each round. It is a much harder and challenging task. We will try to implement it in the next step. </p>

<h2>
<a id="contact" class="anchor" href="#contact" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Contact</h2>

<p>Chaofan Yu           <a href="chaofanyu2015@gmail.com">chaofanyu2015@gmail.com</a></p>

<p>Fei Zhao             <a href="feizhao2015@u.northwestern.edu">feizhao2015@u.northwestern.edu</a></p>

<p>Yuntuo Wang          <a href="yuntuowang2015@u.northwestern.edu">yuntuowang2015@u.northwestern.edu</a></p>

<p>Qian Wang            <a href="qianwang2015@u.northwestern.edu">qianwang2015@u.northwestern.edu</a></p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/yuntuowang/AI_Five-in-row">Magical Gomoku</a> is maintained by <a href="https://github.com/yuntuowang">yuntuowang</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
